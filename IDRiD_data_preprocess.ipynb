{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2436b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "# Note: TensorFlow/Keras imports are here as they'll be needed for the next step (EfficientNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0a7f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentations(image):\n",
    "    \"\"\"\n",
    "    Applies a random sequence of medical-safe augmentations (rotation, flip, brightness, zoom).\n",
    "    \"\"\"\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "\n",
    "    # 1. Random Flip (Horizontal/Vertical)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = cv2.flip(image, 0)\n",
    "\n",
    "    # 2. Small Random Rotation (¬±15 degrees)\n",
    "    angle = np.random.uniform(-15, 15)\n",
    "    M_rot = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    image = cv2.warpAffine(image, M_rot, (w, h), borderMode=cv2.BORDER_CONSTANT)\n",
    "\n",
    "    # 3. Random Brightness Adjustment (¬±20%)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    brightness_factor = np.random.uniform(0.8, 1.2)\n",
    "    hsv[:, :, 2] = np.clip(hsv[:, :, 2] * brightness_factor, 0, 255).astype(np.uint8)\n",
    "    image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    # 4. Small Random Zoom/Scale (0.9 to 1.1)\n",
    "    scale = np.random.uniform(0.9, 1.1)\n",
    "    M_scale = np.array([\n",
    "        [scale, 0, 0], \n",
    "        [0, scale, 0]\n",
    "    ], dtype=np.float32)\n",
    "    M_scale[0, 2] = (1 - scale) * center[0]\n",
    "    M_scale[1, 2] = (1 - scale) * center[1]\n",
    "    image = cv2.warpAffine(image, M_scale, (w, h), borderMode=cv2.BORDER_CONSTANT)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# MODIFIED: load_and_map_labels function to ISOLATE TEST SAMPLES\n",
    "# -------------------------------------------------------------------------------\n",
    "def load_and_map_labels(csv_path, test_keyword='test'):\n",
    "    \"\"\"\n",
    "    Loads the DR grading CSV and splits the data into train/val and test sets\n",
    "    based on the presence of a keyword (e.g., 'test') in the image name.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (df_train_val, df_test) DataFrames\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=['id_code', 'diagnosis'])\n",
    "        df.rename(columns={'id_code': 'Image_Name', 'diagnosis': 'DR_Grade'}, inplace=True)\n",
    "        df['DR_Grade'] = df['DR_Grade'].astype(int) \n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded {len(df)} total labels from CSV.\")\n",
    "\n",
    "        # Isolate Test Samples: Assuming images meant for testing have a keyword like 'test'\n",
    "        # Since the IDRiD files usually don't contain 'test' keywords in the training CSV, \n",
    "        # this logic is set up to handle it if such files are present, or to use \n",
    "        # a standard random split if no keyword is found.\n",
    "        \n",
    "        # Check if the official IDRiD test set structure is in place (e.g., separate files)\n",
    "        # Assuming for now your CSV is the only source.\n",
    "        \n",
    "        # If your 'test' images are identified by ID, you would update this condition\n",
    "        df_test = df[df['Image_Name'].str.contains(test_keyword, case=False, na=False)]\n",
    "        df_train_val = df[~df['Image_Name'].str.contains(test_keyword, case=False, na=False)]\n",
    "\n",
    "        # Secondary split: If no 'test' images are found, reserve 10% randomly for testing\n",
    "        if len(df_test) == 0:\n",
    "            print(\"‚ö†Ô∏è No images with 'test' keyword found. Performing a 90/10 split for Train/Val vs. Test.\")\n",
    "            # Use stratify on the diagnosis to maintain class balance in the test set\n",
    "            df_train_val, df_test = train_test_split(\n",
    "                df, \n",
    "                test_size=0.1, \n",
    "                random_state=42, \n",
    "                stratify=df['DR_Grade']\n",
    "            )\n",
    "        \n",
    "        print(f\"   - Training/Validation Set Size: {len(df_train_val)}\")\n",
    "        print(f\"   - Test Set Size: {len(df_test)}\")\n",
    "        print(f\"Distribution of Grades (Train/Val):\\n{df_train_val['DR_Grade'].value_counts().sort_index()}\")\n",
    "        \n",
    "        return df_train_val, df_test\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during label loading and splitting: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Modified: preprocess_and_augment function now handles both train/val and test logic\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "def preprocess_and_augment(df_labels, images_folder_path, target_size=(224, 224), target_max_samples=156, augment_flag=True):\n",
    "    \"\"\"\n",
    "    Preprocesses, and optionally applies augmentation/oversampling.\n",
    "    \"\"\"\n",
    "    image_data_list = []\n",
    "    \n",
    "    # Only calculate oversampling strategy if augmentation is enabled (for train/val set)\n",
    "    if augment_flag:\n",
    "        grade_counts = df_labels['DR_Grade'].value_counts().to_dict()\n",
    "        needed_augmentations = {grade: max(0, target_max_samples - count) \n",
    "                                for grade, count in grade_counts.items()}\n",
    "        print(\"\\n--- Oversampling Strategy ---\")\n",
    "        print(f\"Target Samples per Grade: {target_max_samples}\")\n",
    "        print(f\"Needed augmentations (per grade total): {needed_augmentations}\")\n",
    "    else:\n",
    "        # For the test set, no oversampling needed\n",
    "        needed_augmentations = {grade: 0 for grade in df_labels['DR_Grade'].unique()}\n",
    "\n",
    "\n",
    "    total_augmentations_applied = 0\n",
    "    \n",
    "    for grade in sorted(df_labels['DR_Grade'].unique()):\n",
    "        grade_df = df_labels[df_labels['DR_Grade'] == grade]\n",
    "        current_count = len(grade_df)\n",
    "        \n",
    "        augmentation_multiplier = 0\n",
    "        if augment_flag and current_count < target_max_samples and current_count > 0:\n",
    "            augmentation_multiplier = int(np.ceil(needed_augmentations[grade] / current_count))\n",
    "        \n",
    "        for index, row in grade_df.iterrows():\n",
    "            img_name_id = row['Image_Name']\n",
    "            dr_grade = row['DR_Grade']\n",
    "            img_path = os.path.join(images_folder_path, img_name_id + '.jpg')\n",
    "            \n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is None:\n",
    "                # Still handle the image read errors\n",
    "                # print(f\"‚ö†Ô∏è Warning: Could not read image {img_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            # 1. ORIGINAL image\n",
    "            # Store the data with a unique filename identifier for the final saving step\n",
    "            image_data_list.append({\n",
    "                'image': img_resized / 255.0, # Normalized image array\n",
    "                'label': dr_grade,           # Integer label\n",
    "                'file_id': f\"{img_name_id}_grade_{dr_grade}_original\"\n",
    "            })\n",
    "            \n",
    "            # 2. Generate AUGMENTED copies (only if augment_flag is True)\n",
    "            if augment_flag:\n",
    "                for i in range(augmentation_multiplier):\n",
    "                    augmented_img = apply_augmentations(img_resized.copy()) \n",
    "                    \n",
    "                    image_data_list.append({\n",
    "                        'image': augmented_img / 255.0,\n",
    "                        'label': dr_grade,\n",
    "                        'file_id': f\"{img_name_id}_grade_{dr_grade}_aug{i+1}\"\n",
    "                    })\n",
    "                    total_augmentations_applied += 1\n",
    "    \n",
    "    # 3. Final Array Conversion\n",
    "    X = np.array([d['image'] for d in image_data_list], dtype='float32')\n",
    "    y_int = np.array([d['label'] for d in image_data_list], dtype='int32')\n",
    "    y_onehot = to_categorical(y_int, num_classes=5)\n",
    "    file_ids = np.array([d['file_id'] for d in image_data_list])\n",
    "    \n",
    "    print(f\"\\n--- Preprocessing Summary ({'Augmented' if augment_flag else 'Test'}) ---\")\n",
    "    print(f\"Total processed samples: {len(X)}\")\n",
    "    if augment_flag:\n",
    "        print(f\"Total augmentations created: {total_augmentations_applied}\")\n",
    "    print(f\"Final Distribution of Grades:\\n{pd.Series(y_int).value_counts().sort_index()}\")\n",
    "    \n",
    "    return X, y_onehot, file_ids, y_int\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# save_split_images function remains unchanged (saves images from arrays to disk)\n",
    "# -------------------------------------------------------------------------------\n",
    "def save_split_images(X_set, y_int_set, file_id_set, output_base_path, folder_name):\n",
    "    \"\"\"Saves the image arrays to the specified train/validation/test folder.\"\"\"\n",
    "    target_folder = os.path.join(output_base_path, folder_name)\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nSaving {len(X_set)} images to: {target_folder}\")\n",
    "    \n",
    "    for i, (img_array, grade, file_id) in enumerate(zip(X_set, y_int_set, file_id_set)):\n",
    "        img_denorm = (img_array * 255.0).astype(np.uint8)\n",
    "        save_path = os.path.join(target_folder, f\"{file_id}.png\")\n",
    "        cv2.imwrite(save_path, img_denorm)\n",
    "        \n",
    "    print(f\"Successfully saved {len(X_set)} files to {folder_name} folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2986595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded 455 total labels from CSV.\n",
      "   - Training/Validation Set Size: 375\n",
      "   - Test Set Size: 80\n",
      "Distribution of Grades (Train/Val):\n",
      "DR_Grade\n",
      "0    114\n",
      "1     17\n",
      "2    125\n",
      "3     64\n",
      "4     55\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Oversampling Strategy ---\n",
      "Target Samples per Grade: 125\n",
      "Needed augmentations (per grade total): {2: 0, 0: 11, 3: 61, 4: 70, 1: 108}\n",
      "\n",
      "--- Preprocessing Summary (Augmented) ---\n",
      "Total processed samples: 782\n",
      "Total augmentations created: 407\n",
      "Final Distribution of Grades:\n",
      "0    228\n",
      "1    136\n",
      "2    125\n",
      "3    128\n",
      "4    165\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Preprocessing Summary (Test) ---\n",
      "Total processed samples: 80\n",
      "Final Distribution of Grades:\n",
      "0    15\n",
      "1     5\n",
      "2    31\n",
      "3    20\n",
      "4     9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Saving Split Images to Disk ---\n",
      "\n",
      "Saving 625 images to: datasets\\Preprocessed_images_output\\train\n",
      "Successfully saved 625 files to train folder.\n",
      "\n",
      "Saving 157 images to: datasets\\Preprocessed_images_output\\validation\n",
      "Successfully saved 157 files to validation folder.\n",
      "\n",
      "Saving 80 images to: datasets\\Preprocessed_images_output\\test\n",
      "Successfully saved 80 files to test folder.\n",
      "\n",
      "--- ‚úÖ Final Data Preparation Complete ---\n",
      "Training Set Size: 625\n",
      "Validation Set Size: 157\n",
      "Test Set Size: 80\n",
      "Images are now saved in: datasets\\Preprocessed_images_output/train, /validation, and /test\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # üìå IMPORTANT: Replace the 'path/to...' parts with your actual local file system structure\n",
    "    # Based on your screenshot:\n",
    "    BASE_PATH = r'datasets\\IDRiD_diabities' # The root folder you showed\n",
    "    \n",
    "    CSV_FILE_PATH = os.path.join(BASE_PATH, 'idrid_labels.csv') \n",
    "    IMAGES_FOLDER_PATH = r'datasets\\IDRiD_diabities\\Imagenes\\Imagenes' # Corrected path for images\n",
    "    OUTPUT_BASE_PATH = r'datasets\\Preprocessed_images_output'\n",
    "\n",
    "    \n",
    "# 1. Load Labels and Split into Train/Val and Test\n",
    "    df_train_val, df_test = load_and_map_labels(CSV_FILE_PATH)\n",
    "\n",
    "    if df_train_val is not None:\n",
    "        \n",
    "        # 2. Process and Augment the Train/Validation Set\n",
    "        X_tv, y_tv, file_ids_tv, y_int_tv = preprocess_and_augment(\n",
    "            df_labels=df_train_val,\n",
    "            images_folder_path=IMAGES_FOLDER_PATH,\n",
    "            target_size=(224, 224), \n",
    "            target_max_samples=125,\n",
    "            augment_flag=True # Enable augmentation/oversampling\n",
    "        )\n",
    "        \n",
    "        # 3. Split the Augmented Data into Training and Validation Arrays (e.g., 80/20)\n",
    "        X_train, X_val, y_train, y_val, file_ids_train, file_ids_val, y_int_train, y_int_val = train_test_split(\n",
    "            X_tv, y_tv, file_ids_tv, y_int_tv, \n",
    "            test_size=0.2, \n",
    "            random_state=42, \n",
    "            stratify=y_int_tv # Stratify using the integer labels of the full balanced set\n",
    "        )\n",
    "        \n",
    "        # 4. Process the Test Set (No Augmentation)\n",
    "        X_test, y_test, file_ids_test, y_int_test = preprocess_and_augment(\n",
    "            df_labels=df_test,\n",
    "            images_folder_path=IMAGES_FOLDER_PATH,\n",
    "            target_size=(224, 224), \n",
    "            target_max_samples=0, # Ignored, but explicit\n",
    "            augment_flag=False # CRUCIAL: Disable augmentation\n",
    "        )\n",
    "\n",
    "        # 5. Save the split images to disk\n",
    "        print(\"\\n--- Saving Split Images to Disk ---\")\n",
    "        \n",
    "        # Save Training and Validation sets\n",
    "        save_split_images(X_train, y_int_train, file_ids_train, OUTPUT_BASE_PATH, 'train')\n",
    "        save_split_images(X_val, y_int_val, file_ids_val, OUTPUT_BASE_PATH, 'validation')\n",
    "        \n",
    "        # Save Test set\n",
    "        save_split_images(X_test, y_int_test, file_ids_test, OUTPUT_BASE_PATH, 'test')\n",
    "\n",
    "\n",
    "        print(\"\\n--- ‚úÖ Final Data Preparation Complete ---\")\n",
    "        print(f\"Training Set Size: {len(X_train)}\")\n",
    "        print(f\"Validation Set Size: {len(X_val)}\")\n",
    "        print(f\"Test Set Size: {len(X_test)}\")\n",
    "        print(f\"Images are now saved in: {OUTPUT_BASE_PATH}/train, /validation, and /test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d23a38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow device in use: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "‚úÖ GPU memory growth set.\n",
      "\n",
      "--- Phase 1: Training Classification Head (Frozen Base) ---\n",
      "Epoch 1/20\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.6384 - accuracy: 0.2656\n",
      "Epoch 1: val_loss improved from inf to 1.59669, saving model to efficientnet_idrid_best_weights_only.h5\n",
      "40/40 [==============================] - 8s 88ms/step - loss: 1.6384 - accuracy: 0.2656 - val_loss: 1.5967 - val_accuracy: 0.2102\n",
      "Epoch 2/20\n",
      "39/40 [============================>.] - ETA: 0s - loss: 1.6329 - accuracy: 0.2397\n",
      "Epoch 2: val_loss did not improve from 1.59669\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 1.6270 - accuracy: 0.2416 - val_loss: 1.6057 - val_accuracy: 0.2102\n",
      "Epoch 3/20\n",
      "39/40 [============================>.] - ETA: 0s - loss: 1.6404 - accuracy: 0.2365\n",
      "Epoch 3: val_loss did not improve from 1.59669\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 1.6387 - accuracy: 0.2400 - val_loss: 1.6001 - val_accuracy: 0.2930\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.6432 - accuracy: 0.2352\n",
      "Epoch 4: val_loss did not improve from 1.59669\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 1.6432 - accuracy: 0.2352 - val_loss: 1.5984 - val_accuracy: 0.2930\n",
      "Epoch 5/20\n",
      "39/40 [============================>.] - ETA: 0s - loss: 1.6306 - accuracy: 0.2693\n",
      "Epoch 5: val_loss improved from 1.59669 to 1.59324, saving model to efficientnet_idrid_best_weights_only.h5\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.6326 - accuracy: 0.2672 - val_loss: 1.5932 - val_accuracy: 0.2930\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.6362 - accuracy: 0.2432\n",
      "Epoch 6: val_loss did not improve from 1.59324\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 1.6362 - accuracy: 0.2432 - val_loss: 1.5959 - val_accuracy: 0.2930\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.6238 - accuracy: 0.2560\n",
      "Epoch 7: val_loss improved from 1.59324 to 1.58679, saving model to efficientnet_idrid_best_weights_only.h5\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.6238 - accuracy: 0.2560 - val_loss: 1.5868 - val_accuracy: 0.2930\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.6290 - accuracy: 0.2592\n",
      "Epoch 8: val_loss did not improve from 1.58679\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 1.6290 - accuracy: 0.2592 - val_loss: 1.6310 - val_accuracy: 0.2930\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.6183 - accuracy: 0.2416\n",
      "Epoch 9: val_loss did not improve from 1.58679\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 1.6183 - accuracy: 0.2416 - val_loss: 1.5913 - val_accuracy: 0.2930\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.6189 - accuracy: 0.2304\n",
      "Epoch 10: val_loss did not improve from 1.58679\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 1.6189 - accuracy: 0.2304 - val_loss: 1.5949 - val_accuracy: 0.2930\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.6223 - accuracy: 0.2432\n",
      "Epoch 11: val_loss did not improve from 1.58679\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 1.6223 - accuracy: 0.2432 - val_loss: 1.5930 - val_accuracy: 0.2930\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.6156 - accuracy: 0.2640\n",
      "Epoch 12: val_loss did not improve from 1.58679\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 1.6156 - accuracy: 0.2640 - val_loss: 1.5972 - val_accuracy: 0.2930\n",
      "Epoch 13/20\n",
      "39/40 [============================>.] - ETA: 0s - loss: 1.6466 - accuracy: 0.2660\n",
      "Epoch 13: val_loss did not improve from 1.58679\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 1.6442 - accuracy: 0.2672 - val_loss: 1.5877 - val_accuracy: 0.2930\n",
      "Epoch 14/20\n",
      "39/40 [============================>.] - ETA: 0s - loss: 1.6159 - accuracy: 0.2627\n",
      "Epoch 14: val_loss did not improve from 1.58679\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 1.6132 - accuracy: 0.2608 - val_loss: 1.6042 - val_accuracy: 0.2930\n",
      "Epoch 15/20\n",
      "39/40 [============================>.] - ETA: 0s - loss: 1.6253 - accuracy: 0.2693\n",
      "Epoch 15: val_loss did not improve from 1.58679\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "40/40 [==============================] - 2s 52ms/step - loss: 1.6276 - accuracy: 0.2640 - val_loss: 1.5956 - val_accuracy: 0.2930\n",
      "Epoch 15: early stopping\n",
      "\n",
      "--- Phase 2: Fine-Tuning (Unfreeze and Retrain) ---\n",
      "Reduced batch size to 8 for fine-tuning to prevent OOM errors.\n",
      "Epoch 16/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.6661 - accuracy: 0.2240\n",
      "Epoch 16: val_loss did not improve from 1.58679\n",
      "79/79 [==============================] - 17s 112ms/step - loss: 1.6661 - accuracy: 0.2240 - val_loss: 1.6591 - val_accuracy: 0.1592\n",
      "Epoch 17/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.5661 - accuracy: 0.2960\n",
      "Epoch 17: val_loss did not improve from 1.58679\n",
      "79/79 [==============================] - 7s 94ms/step - loss: 1.5661 - accuracy: 0.2960 - val_loss: 1.6592 - val_accuracy: 0.2038\n",
      "Epoch 18/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.4722 - accuracy: 0.3632\n",
      "Epoch 18: val_loss did not improve from 1.58679\n",
      "79/79 [==============================] - 8s 95ms/step - loss: 1.4722 - accuracy: 0.3632 - val_loss: 1.6512 - val_accuracy: 0.1975\n",
      "Epoch 19/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.4014 - accuracy: 0.4224\n",
      "Epoch 19: val_loss did not improve from 1.58679\n",
      "79/79 [==============================] - 8s 95ms/step - loss: 1.4014 - accuracy: 0.4224 - val_loss: 1.6313 - val_accuracy: 0.2038\n",
      "Epoch 20/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.3511 - accuracy: 0.4672\n",
      "Epoch 20: val_loss did not improve from 1.58679\n",
      "79/79 [==============================] - 7s 94ms/step - loss: 1.3511 - accuracy: 0.4672 - val_loss: 1.6384 - val_accuracy: 0.1911\n",
      "Epoch 21/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.2756 - accuracy: 0.5232\n",
      "Epoch 21: val_loss did not improve from 1.58679\n",
      "79/79 [==============================] - 8s 95ms/step - loss: 1.2756 - accuracy: 0.5232 - val_loss: 1.5898 - val_accuracy: 0.2102\n",
      "Epoch 22/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.2111 - accuracy: 0.5616\n",
      "Epoch 22: val_loss improved from 1.58679 to 1.51577, saving model to efficientnet_idrid_best_weights_only.h5\n",
      "79/79 [==============================] - 8s 98ms/step - loss: 1.2111 - accuracy: 0.5616 - val_loss: 1.5158 - val_accuracy: 0.3248\n",
      "Epoch 23/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.1756 - accuracy: 0.5696\n",
      "Epoch 23: val_loss improved from 1.51577 to 1.41533, saving model to efficientnet_idrid_best_weights_only.h5\n",
      "79/79 [==============================] - 8s 100ms/step - loss: 1.1756 - accuracy: 0.5696 - val_loss: 1.4153 - val_accuracy: 0.4268\n",
      "Epoch 24/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.1203 - accuracy: 0.5712\n",
      "Epoch 24: val_loss did not improve from 1.41533\n",
      "79/79 [==============================] - 8s 95ms/step - loss: 1.1203 - accuracy: 0.5712 - val_loss: 1.4966 - val_accuracy: 0.2994\n",
      "Epoch 25/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.0707 - accuracy: 0.6032\n",
      "Epoch 25: val_loss did not improve from 1.41533\n",
      "79/79 [==============================] - 7s 94ms/step - loss: 1.0707 - accuracy: 0.6032 - val_loss: 1.4544 - val_accuracy: 0.3567\n",
      "Epoch 26/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.0634 - accuracy: 0.5984\n",
      "Epoch 26: val_loss did not improve from 1.41533\n",
      "79/79 [==============================] - 7s 94ms/step - loss: 1.0634 - accuracy: 0.5984 - val_loss: 1.5166 - val_accuracy: 0.3121\n",
      "Epoch 27/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.0032 - accuracy: 0.6608\n",
      "Epoch 27: val_loss improved from 1.41533 to 1.38826, saving model to efficientnet_idrid_best_weights_only.h5\n",
      "79/79 [==============================] - 8s 97ms/step - loss: 1.0032 - accuracy: 0.6608 - val_loss: 1.3883 - val_accuracy: 0.4268\n",
      "Epoch 28/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.9676 - accuracy: 0.6480\n",
      "Epoch 28: val_loss did not improve from 1.38826\n",
      "79/79 [==============================] - 8s 95ms/step - loss: 0.9676 - accuracy: 0.6480 - val_loss: 1.5086 - val_accuracy: 0.3121\n",
      "Epoch 29/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.9449 - accuracy: 0.6544\n",
      "Epoch 29: val_loss improved from 1.38826 to 1.35575, saving model to efficientnet_idrid_best_weights_only.h5\n",
      "79/79 [==============================] - 8s 98ms/step - loss: 0.9449 - accuracy: 0.6544 - val_loss: 1.3558 - val_accuracy: 0.4522\n",
      "Epoch 30/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.8987 - accuracy: 0.6608\n",
      "Epoch 30: val_loss did not improve from 1.35575\n",
      "79/79 [==============================] - 7s 94ms/step - loss: 0.8987 - accuracy: 0.6608 - val_loss: 1.5447 - val_accuracy: 0.3248\n",
      "Epoch 31/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.8468 - accuracy: 0.7104\n",
      "Epoch 31: val_loss did not improve from 1.35575\n",
      "79/79 [==============================] - 7s 95ms/step - loss: 0.8468 - accuracy: 0.7104 - val_loss: 1.4223 - val_accuracy: 0.3185\n",
      "Epoch 32/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.8648 - accuracy: 0.6736\n",
      "Epoch 32: val_loss improved from 1.35575 to 1.07081, saving model to efficientnet_idrid_best_weights_only.h5\n",
      "79/79 [==============================] - 8s 102ms/step - loss: 0.8648 - accuracy: 0.6736 - val_loss: 1.0708 - val_accuracy: 0.5669\n",
      "Epoch 33/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.8117 - accuracy: 0.7248\n",
      "Epoch 33: val_loss did not improve from 1.07081\n",
      "79/79 [==============================] - 8s 100ms/step - loss: 0.8117 - accuracy: 0.7248 - val_loss: 1.1736 - val_accuracy: 0.5350\n",
      "Epoch 34/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.8027 - accuracy: 0.7104\n",
      "Epoch 34: val_loss did not improve from 1.07081\n",
      "79/79 [==============================] - 8s 96ms/step - loss: 0.8027 - accuracy: 0.7104 - val_loss: 1.2358 - val_accuracy: 0.4904\n",
      "Epoch 35/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.7512 - accuracy: 0.7392\n",
      "Epoch 35: val_loss did not improve from 1.07081\n",
      "79/79 [==============================] - 8s 95ms/step - loss: 0.7512 - accuracy: 0.7392 - val_loss: 1.1360 - val_accuracy: 0.5414\n",
      "Epoch 36/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.7328 - accuracy: 0.7424\n",
      "Epoch 36: val_loss did not improve from 1.07081\n",
      "79/79 [==============================] - 8s 95ms/step - loss: 0.7328 - accuracy: 0.7424 - val_loss: 1.4549 - val_accuracy: 0.4013\n",
      "Epoch 37/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.7162 - accuracy: 0.7616\n",
      "Epoch 37: val_loss improved from 1.07081 to 1.01286, saving model to efficientnet_idrid_best_weights_only.h5\n",
      "79/79 [==============================] - 8s 99ms/step - loss: 0.7162 - accuracy: 0.7616 - val_loss: 1.0129 - val_accuracy: 0.5987\n",
      "Epoch 38/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.7178 - accuracy: 0.7376\n",
      "Epoch 38: val_loss did not improve from 1.01286\n",
      "79/79 [==============================] - 8s 95ms/step - loss: 0.7178 - accuracy: 0.7376 - val_loss: 1.0349 - val_accuracy: 0.5924\n",
      "Epoch 39/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.7043 - accuracy: 0.7584\n",
      "Epoch 39: val_loss did not improve from 1.01286\n",
      "79/79 [==============================] - 7s 94ms/step - loss: 0.7043 - accuracy: 0.7584 - val_loss: 1.3536 - val_accuracy: 0.4841\n",
      "Epoch 40/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.6401 - accuracy: 0.7792\n",
      "Epoch 40: val_loss did not improve from 1.01286\n",
      "79/79 [==============================] - 8s 95ms/step - loss: 0.6401 - accuracy: 0.7792 - val_loss: 1.2989 - val_accuracy: 0.4777\n",
      "Epoch 41/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.6305 - accuracy: 0.7920\n",
      "Epoch 41: val_loss did not improve from 1.01286\n",
      "79/79 [==============================] - 8s 95ms/step - loss: 0.6305 - accuracy: 0.7920 - val_loss: 1.3125 - val_accuracy: 0.4777\n",
      "Epoch 42/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.6190 - accuracy: 0.7872\n",
      "Epoch 42: val_loss did not improve from 1.01286\n",
      "79/79 [==============================] - 8s 96ms/step - loss: 0.6190 - accuracy: 0.7872 - val_loss: 1.0604 - val_accuracy: 0.5287\n",
      "Epoch 43/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.6216 - accuracy: 0.7920\n",
      "Epoch 43: val_loss did not improve from 1.01286\n",
      "79/79 [==============================] - 8s 95ms/step - loss: 0.6216 - accuracy: 0.7920 - val_loss: 1.2540 - val_accuracy: 0.5032\n",
      "Epoch 44/50\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.5879 - accuracy: 0.8064\n",
      "Epoch 44: val_loss did not improve from 1.01286\n",
      "79/79 [==============================] - 8s 97ms/step - loss: 0.5879 - accuracy: 0.8064 - val_loss: 1.2256 - val_accuracy: 0.5032\n",
      "Epoch 45/50\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.5878 - accuracy: 0.7821\n",
      "Epoch 45: val_loss did not improve from 1.01286\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "79/79 [==============================] - 8s 98ms/step - loss: 0.5896 - accuracy: 0.7808 - val_loss: 1.1714 - val_accuracy: 0.5414\n",
      "Epoch 45: early stopping\n",
      "\n",
      "--- Final Evaluation on Test Set ---\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 1.2893 - accuracy: 0.4000\n",
      "\n",
      "Final Test Accuracy: 0.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 81). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: efficientnet_idrid_final_savedmodel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: efficientnet_idrid_final_savedmodel\\assets\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 213\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# Save the final model in the SavedModel format (robust against serialization errors)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m SAVE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mefficientnet_idrid_final_savedmodel\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 213\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müéâ Model saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAVE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m folder (SavedModel format).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\imagedetection_project\\gpuvenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\imagedetection_project\\gpuvenv\\lib\\json\\encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32me:\\imagedetection_project\\gpuvenv\\lib\\json\\encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# --- 1. GPU/TF Environment Setup ---\n",
    "print(\"TensorFlow device in use:\", tf.config.list_physical_devices())\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Allow memory growth for efficient GPU memory usage\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"‚úÖ GPU memory growth set.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not set GPU memory growth: {e}\")\n",
    "\n",
    "# --- 2. Data Generator Class ---\n",
    "\n",
    "class ImageBatchGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Keras Sequence to generate batches of images and labels from disk, \n",
    "    efficiently handling large datasets like the augmented IDRiD set.\n",
    "    \"\"\"\n",
    "    def __init__(self, folder_path, batch_size=16, num_classes=5, shuffle=True):\n",
    "        # List all PNG files in the directory\n",
    "        self.file_list = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n",
    "        self.folder_path = folder_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        if not self.file_list:\n",
    "             raise FileNotFoundError(f\"‚ùå No .png files found in the folder: {folder_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of batches per epoch\n",
    "        return int(np.ceil(len(self.file_list) / self.batch_size))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate indices for the batch\n",
    "        batch_files = self.file_list[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        images, labels = [], []\n",
    "        \n",
    "        for fname in batch_files:\n",
    "            # Load image and convert from BGR (cv2 default) to RGB\n",
    "            img = cv2.imread(os.path.join(self.folder_path, fname))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Normalize to [0, 1]\n",
    "            images.append(img / 255.0)\n",
    "            \n",
    "            # Extract grade from filename: ..._grade_{label}_...\n",
    "            try:\n",
    "                # Assuming filename format is 'ImageName_grade_X_suffix.png'\n",
    "                grade_str = fname.split('_grade_')[1].split('_')[0]\n",
    "                grade = int(grade_str)\n",
    "            except IndexError:\n",
    "                # Handle cases where the filename format might be unexpected\n",
    "                print(f\"‚ö†Ô∏è Could not parse grade from filename: {fname}. Assuming grade 0.\")\n",
    "                grade = 0\n",
    "                \n",
    "            labels.append(grade)\n",
    "            \n",
    "        images = np.array(images, dtype='float32')\n",
    "        # One-hot encode the labels\n",
    "        labels = tf.keras.utils.to_categorical(labels, num_classes=self.num_classes)\n",
    "        return images, labels\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle file list after each epoch\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.file_list)\n",
    "\n",
    "# --- 3. Model Building Function ---\n",
    "\n",
    "def build_efficientnet_model(input_shape, num_classes):\n",
    "    \"\"\"Builds the EfficientNetB0 model with a custom classification head.\"\"\"\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    \n",
    "    # 1. Load pre-trained base model\n",
    "    base_model = EfficientNetB0(\n",
    "        include_top=False, \n",
    "        weights='imagenet', \n",
    "        input_tensor=input_tensor\n",
    "    )\n",
    "    # Freeze the base model for Phase 1\n",
    "    base_model.trainable = False \n",
    "    \n",
    "    # 2. Add custom classification head\n",
    "    x = GlobalAveragePooling2D(name=\"avg_pool\")(base_model.output)\n",
    "    x = Dropout(0.5)(x) \n",
    "    output = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "# --- 4. Main Execution Block ---\n",
    "\n",
    "# --- DATA PATHS ---\n",
    "# NOTE: Ensure these paths match where you saved your preprocessed data\n",
    "# Example path structure assumed: BASE_PATH/Preprocessed_images_output/{train,validation,test}\n",
    "OUTPUT_BASE_PATH = r'datasets\\Preprocessed_images_output' \n",
    "\n",
    "train_folder = os.path.join(OUTPUT_BASE_PATH, 'train')\n",
    "val_folder = os.path.join(OUTPUT_BASE_PATH, 'validation')\n",
    "test_folder = os.path.join(OUTPUT_BASE_PATH, 'test')\n",
    "\n",
    "batch_size = 16\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# Create Generators\n",
    "train_gen = ImageBatchGenerator(train_folder, batch_size=batch_size, shuffle=True)\n",
    "val_gen = ImageBatchGenerator(val_folder, batch_size=batch_size, shuffle=False)\n",
    "test_gen = ImageBatchGenerator(test_folder, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Build Model\n",
    "model = build_efficientnet_model(INPUT_SHAPE, NUM_CLASSES)\n",
    "\n",
    "# --- Callbacks ---\n",
    "# Saving ONLY weights to avoid the JSON serialization/EagerTensor error\n",
    "checkpoint_path = 'efficientnet_idrid_best_weights_only.h5'\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path, \n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=True, \n",
    "    mode='min'\n",
    ")\n",
    "# Patience increased to 8 for more resilient training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=8, \n",
    "    verbose=1, \n",
    "    mode='min', \n",
    "    restore_best_weights=True\n",
    ")\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# PHASE 1: Train Classification Head (Frozen Base)\n",
    "# --------------------------------------------------------------------------------\n",
    "print(\"\\n--- Phase 1: Training Classification Head (Frozen Base) ---\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3), # Higher LR (1e-3) for new layers\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=20, # Initial training epochs increased\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# PHASE 2: Fine-Tuning (Unfreeze and Retrain)\n",
    "# --------------------------------------------------------------------------------\n",
    "print(\"\\n--- Phase 2: Fine-Tuning (Unfreeze and Retrain) ---\")\n",
    "\n",
    "# --- CRITICAL FIX: Create new generators with a smaller batch size (e.g., 8) ---\n",
    "fine_tune_batch_size = 8 \n",
    "print(f\"Reduced batch size to {fine_tune_batch_size} for fine-tuning to prevent OOM errors.\")\n",
    "\n",
    "train_gen_ft = ImageBatchGenerator(train_folder, batch_size=fine_tune_batch_size, shuffle=True)\n",
    "val_gen_ft = ImageBatchGenerator(val_folder, batch_size=fine_tune_batch_size, shuffle=False)\n",
    "\n",
    "# Unfreeze the base model\n",
    "model.trainable = True\n",
    "\n",
    "# Load the best weights from Phase 1 before starting fine-tuning\n",
    "# ... (loading weights code) ...\n",
    "    \n",
    "# Re-compile with a much lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5), # Very low LR (1e-5) for fine-tuning\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Use the new, smaller generators for the fit call\n",
    "history_finetune = model.fit(\n",
    "    train_gen_ft,             # <-- Use the smaller batch size generator\n",
    "    validation_data=val_gen_ft, # <-- Use the smaller batch size generator\n",
    "    epochs=50, \n",
    "    initial_epoch=history.epoch[-1] + 1 if history.epoch else 0,\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 5. FINAL EVALUATION AND SAVING\n",
    "# --------------------------------------------------------------------------------\n",
    "print(\"\\n--- Final Evaluation on Test Set ---\")\n",
    "# Load the overall best weights saved during both phases\n",
    "model.load_weights(checkpoint_path) \n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_gen, verbose=1)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Save the final model in the SavedModel format (robust against serialization errors)\n",
    "SAVE_PATH = 'efficientnet_idrid_final_savedmodel'\n",
    "model.save(SAVE_PATH)\n",
    "print(f\"üéâ Model saved to {SAVE_PATH} folder (SavedModel format).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpuvenv)",
   "language": "python",
   "name": "gpuvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
